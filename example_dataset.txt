As word2vec is a neural network, it benefits from very large datasets. 
The Kaggle dataset is 50,000 reviews * ~5 sentences per review, so about a quarter million sentences. 
As they note, they get approximately the same results using bag of words and word2vec. 
One thing which is of note, since the review data comes from the internet, the sentences are much more loosely structured than what you would encounter in a corpus of newspapers, which typically go through grammatical review. 
A great dataset for training word2vec on structured language is the wikipedia dataset.