{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "vocab = set(words)\n",
    "embedding_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "This is the forward graph\n",
    "```\n",
    "embedding --(@ linear)--> x --(softmax)--> o --(criterion)--> loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset import SentencesDataset\n",
    "\n",
    "def softmax(array):\n",
    "    array = np.exp(array)\n",
    "    return array / np.sum(array, axis=1, keepdims=True)\n",
    "\n",
    "class Word2Vec():\n",
    "    def __init__(self, vocab, embedding_size, learning_rate=1.):\n",
    "        self.vocab = vocab\n",
    "        self.embedding_size = embedding_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.word_to_index = { word: i for i, word in enumerate(self.vocab)}\n",
    "        self.index_to_word = { i: word for word, i in self.word_to_index.items()}\n",
    "        \n",
    "        self.embedding = np.random.randn(self.vocab_size, self.embedding_size)\n",
    "        self.linear = np.random.randn(self.embedding_size, self.vocab_size)\n",
    "        \n",
    "    def one_hot_encod(self, output_indices):\n",
    "        one_hot = np.zeros((len(output_indices), self.vocab_size))\n",
    "        one_hot[np.arange(len(output_indices)), output_indices] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def forward(self, input_indices):\n",
    "        embeddings = self.embedding[input_indices, :]\n",
    "\n",
    "        return embeddings, softmax(embeddings @ self.linear)\n",
    "    \n",
    "    def criterion(self, y, outputs):\n",
    "        return -1/self.vocab_size*np.sum(y*np.log(outputs), axis=1)\n",
    "    \n",
    "    def step(self, input_indices, output_indices):\n",
    "        y = self.one_hot_encod(output_indices)\n",
    "        embeddings, outputs = self.forward(input_indices)\n",
    "\n",
    "        loss = self.criterion(y, outputs)\n",
    "\n",
    "        dL_dx = 1/self.vocab_size*(outputs-y)\n",
    "\n",
    "        grad_linear = embeddings.T @ dL_dx\n",
    "        self.linear -= self.learning_rate*grad_linear\n",
    "        \n",
    "        grad_embedding = dL_dx @ self.linear.T\n",
    "        self.embedding[input_indices, :] -= self.learning_rate*grad_embedding\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_indices = [1,2,2]\n",
    "output_indices = [0,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(vocab, embedding_size)\n",
    "e, o = word2vec.forward(input_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = word2vec.one_hot_encod(output_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4615007761471727"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.step(input_indices, output_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510014487379439\n",
      "0.37416519566696105\n",
      "0.20625483228547825\n",
      "0.14466301387770514\n",
      "0.10419005293889565\n",
      "0.07375236755664163\n",
      "0.051896978184955714\n",
      "0.03721310284640155\n",
      "0.02768196606628731\n",
      "0.02144456736252744\n",
      "0.017225185752799594\n",
      "0.014254101085234727\n",
      "0.012080590563486301\n",
      "0.010436555241408924\n",
      "0.009157255692508007\n",
      "0.008137709624571842\n",
      "0.007308621652623333\n",
      "0.006622759563574707\n",
      "0.006046993205997484\n",
      "0.005557486688026594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.89855561e-06, 1.09933664e-04, 9.79568607e-01, 8.87112328e-03,\n",
       "        1.50245225e-03, 4.95329737e-03, 3.96035458e-03, 1.02733371e-03],\n",
       "       [8.83148284e-04, 1.84495314e-02, 1.05908505e-02, 9.40311355e-01,\n",
       "        6.34542699e-03, 2.42105704e-03, 8.00264324e-03, 1.29959878e-02]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_indices, output_indices = [0,1], [2,3]\n",
    "\n",
    "for _ in range(20):\n",
    "    print(word2vec.step(input_indices, output_indices))\n",
    "_, outputs = word2vec.forward(input_indices)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class WordPairsDataset:\n",
    "    \"\"\"\n",
    "    Very simple dataset composed of pairs of words\n",
    "    \"\"\"\n",
    "    def __init__(self, data_filename, window_size=5):\n",
    "        \"\"\"\n",
    "        window_size must be odd\n",
    "        \"\"\"\n",
    "        super(WordPairsDataset, self).__init__()\n",
    "        self.pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*') # Tokens containing an alpha symbol\n",
    "        self.padding = window_size // 2\n",
    "        \n",
    "        with open(data_filename, \"r\") as f:\n",
    "            self.sentences = [l for l in f]\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.preprocess()\n",
    "        \n",
    "        self.pairs = []\n",
    "        for sentence in self.sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for delta in range(-self.padding, self.padding+1):\n",
    "                    if delta != 0 and i + delta >= 0 and i + delta < len(sentence):\n",
    "                        self.pairs.append((word, sentence[i + delta]))\n",
    "                    \n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        return self.pattern.findall(sentence.lower())\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.sentences = list(map(self.tokenize, self.sentences))\n",
    "        self.sentences = list(filter(lambda l: len(l) >= self.window_size, self.sentences))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.pairs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, dataset, batch_size, word_to_index):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.word_to_index = word_to_index\n",
    "        \n",
    "        self.n_batches = ceil(len(self.dataset) / batch_size)\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        input_indices, output_indices = [], []\n",
    "        \n",
    "        for input_word, output_word in self.dataset[self.current_index:self.current_index+self.batch_size]:\n",
    "            input_indices.append(self.word_to_index[input_word])\n",
    "            output_indices.append(self.word_to_index[output_word])\n",
    "            \n",
    "        self.current_index += self.n_batches\n",
    "        if self.current_index < len(self.dataset):\n",
    "            return np.array(input_indices), np.array(output_indices)\n",
    "        else:\n",
    "            self.current_index = 0\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpd = WordPairsDataset(\"example_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "words = reduce(lambda x,y: x+y, wpd.sentences)\n",
    "vocab = set(words)\n",
    "embedding_size = 10\n",
    "\n",
    "word2vec = Word2Vec(vocab, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(wpd, 3, word2vec.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([24, 24, 50]), array([50, 30, 24]))\n",
      "(array([50, 30, 30]), array([18, 24, 50]))\n"
     ]
    }
   ],
   "source": [
    "for x in loader:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('as', 'word2vec'),\n",
       " ('as', 'is'),\n",
       " ('word2vec', 'as'),\n",
       " ('word2vec', 'is'),\n",
       " ('word2vec', 'a'),\n",
       " ('is', 'as'),\n",
       " ('is', 'word2vec'),\n",
       " ('is', 'a'),\n",
       " ('is', 'neural'),\n",
       " ('a', 'word2vec')]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpd.pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
